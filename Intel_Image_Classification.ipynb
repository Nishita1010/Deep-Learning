{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Assignment_2: Intel Image Classification**"
      ],
      "metadata": {
        "id": "IRWv7GdYcg1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Problem Statement:**\n",
        "####Perform experiments with images and check out the effect of batch processing, batch size, image spatial dimension etc. on the image data set. Find out the limitations of your machine in terms of processing the batch of images."
      ],
      "metadata": {
        "id": "C7_tuFTobf8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dataset Description:**\n",
        "\n",
        "This image data set focuses on natural scenes. It was created by Intel for an image\n",
        "classification competition. Approximately 25,000 images are there in the dataset.\n",
        "Images are grouped into categories such as buildings, forests, glaciers, mountains, seas,\n",
        "and streets.\n",
        "Dataset is divided into folders for training, testing, and prediction:\n",
        "\n",
        "* 14,000 training images\n",
        "\n",
        "* 3,000 validation images\n",
        "\n",
        "* 7,000 test images\n",
        "\n",
        "Link to the Dataset:\n",
        "\n",
        "URL: https://www.kaggle.com/datasets/puneet6060/intel-image-classification"
      ],
      "metadata": {
        "id": "XDgdv4uQc-0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Methodology**:\n",
        "In this model, input images of three different spatial dimensions are used and the model is trained by experimenting with two different batch sizes as mentioned below: \n",
        "\n",
        "* Input Image spatial dimension: 32, 64, 256.\n",
        "\n",
        "* Batch sizes: 90, 150\n",
        "\n",
        "#**Outcome**:\n",
        "These experiments have been done to understand and know that how different spatial dimentions and batch processing affects the model, how much time it takes and what can be done to improve it further or make more efficient. \n",
        " "
      ],
      "metadata": {
        "id": "_3gQ40JDdRFm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y2_8jH8MNfvN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing import image\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D \n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F2VX9qZr0nWw"
      },
      "outputs": [],
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VoSSeIhkhOnE"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVjHBgc1gOTt",
        "outputId": "680f1ea1-7fd2-4490-8332-2637330031ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intel-image-classification.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d puneet6060/intel-image-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3_mBtZbqg25H"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/intel-image-classification.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iQ1QPkOA5KjW"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "buildings = glob.glob('/content/seg_test/seg_test/buildings/*.*')\n",
        "forest = glob.glob('/content/seg_test/seg_test/forest/*.*')\n",
        "glacier = glob.glob('/content/seg_test/seg_test/glacier/*.*')\n",
        "mountain = glob.glob('/content/seg_test/seg_test/moutain/*.*')\n",
        "sea = glob.glob('/content/seg_test/seg_test/sea/*.*')\n",
        "street = glob.glob('/content/seg_test/seg_test/street/*.*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Zp_ftoPqtyKP"
      },
      "outputs": [],
      "source": [
        "imgShape = [32,64,256]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoApChdc1lI3"
      },
      "source": [
        "##Image Dimention: 32 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0rdGRNyZ1lI4"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eLfIrJCH1lI4"
      },
      "outputs": [],
      "source": [
        "for i in buildings:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[0],imgShape[0]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(0)\n",
        "for i in forest:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[0],imgShape[0]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(1)\n",
        "for i in glacier:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[0],imgShape[0]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(2)\n",
        "for i in mountain:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[0],imgShape[0]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(3)\n",
        "for i in sea:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[0],imgShape[0]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(2)\n",
        "for i in street:    \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[0],imgShape[0]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CGGDTHAB1lI4"
      },
      "outputs": [],
      "source": [
        "data32 = np.array(data)\n",
        "labels32 = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nMiIqVkp1lI4"
      },
      "outputs": [],
      "source": [
        "x_train32, x_test32, y_train32, y_test32 = train_test_split(data32, labels32, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2n_yRnO1lI4",
        "outputId": "32e77540-a383-41c3-919c-687dee3d4337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1980 train samples\n",
            "495 test samples\n"
          ]
        }
      ],
      "source": [
        "x_train32 = x_train32.astype('float32')\n",
        "x_test32 = x_test32.astype('float32')\n",
        "x_train32 /= 255\n",
        "x_test32 /= 255\n",
        "\n",
        "print(x_train32.shape[0], 'train samples')\n",
        "print(x_test32.shape[0], 'test samples')\n",
        "\n",
        "# Convert labels to categorical format\n",
        "y_train32 = keras.utils.to_categorical(y_train32, 6)\n",
        "y_test32 = keras.utils.to_categorical(y_test32, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEOy5JAe1lI4",
        "outputId": "a76e4e2b-756d-4138-a0d8-afe77571f6e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.convolutional.conv2d.Conv2D at 0x7f21a48e7790>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f21a3b72f70>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7f21a3b727f0>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f21a3b72d30>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7f21a3ab2070>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f21a3ab2760>,\n",
              " <keras.layers.reshaping.flatten.Flatten at 0x7f21a3ab26d0>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21a3ac25e0>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21a3ac29d0>,\n",
              " <keras.layers.regularization.dropout.Dropout at 0x7f2198040910>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21980407c0>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Conv2D(32, (5, 5), input_shape=(32,32,3), activation='relu', padding='same'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model1.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model1.add(Conv2D(200, (5, 5), activation='relu', padding='same'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model1.add(Flatten())\n",
        "\n",
        "\n",
        "model1.add(Dense(300, activation='relu'))\n",
        "model1.add(Dense(200, activation='relu'))\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Dense(6, activation=\"softmax\"))\n",
        "model1.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Dimention 32 and batch size 90"
      ],
      "metadata": {
        "id": "kqmYK73ghJJK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGx0-tnE1lI4",
        "outputId": "d8136d29-8a2f-4a80-b549-96dbe3f2fb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "22/22 [==============================] - 13s 29ms/step - loss: 1.2438 - accuracy: 0.4652 - val_loss: 0.9474 - val_accuracy: 0.6222\n",
            "Epoch 2/15\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.8519 - accuracy: 0.6667 - val_loss: 0.7415 - val_accuracy: 0.7091\n",
            "Epoch 3/15\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.7239 - accuracy: 0.7242 - val_loss: 0.6957 - val_accuracy: 0.7273\n",
            "Epoch 4/15\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.6302 - accuracy: 0.7581 - val_loss: 0.6532 - val_accuracy: 0.7333\n",
            "Epoch 5/15\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.5567 - accuracy: 0.7859 - val_loss: 0.6089 - val_accuracy: 0.7697\n",
            "Epoch 6/15\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.5407 - accuracy: 0.7985 - val_loss: 0.6583 - val_accuracy: 0.7556\n",
            "Epoch 7/15\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.4893 - accuracy: 0.8177 - val_loss: 0.6559 - val_accuracy: 0.7758\n",
            "Epoch 8/15\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.4479 - accuracy: 0.8389 - val_loss: 0.5358 - val_accuracy: 0.8081\n",
            "Epoch 9/15\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3936 - accuracy: 0.8576 - val_loss: 0.4989 - val_accuracy: 0.8141\n",
            "Epoch 10/15\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.3259 - accuracy: 0.8838 - val_loss: 0.5583 - val_accuracy: 0.7879\n",
            "Epoch 11/15\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.3059 - accuracy: 0.8985 - val_loss: 0.6053 - val_accuracy: 0.8182\n",
            "Epoch 12/15\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2698 - accuracy: 0.9020 - val_loss: 0.5227 - val_accuracy: 0.8202\n",
            "Epoch 13/15\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.2660 - accuracy: 0.9045 - val_loss: 0.5768 - val_accuracy: 0.7798\n",
            "Epoch 14/15\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.1739 - accuracy: 0.9455 - val_loss: 0.6170 - val_accuracy: 0.8081\n",
            "Epoch 15/15\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1564 - accuracy: 0.9490 - val_loss: 0.6129 - val_accuracy: 0.7879\n"
          ]
        }
      ],
      "source": [
        "# Compile the model1\n",
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model1\n",
        "history1= model1.fit(x_train32, y_train32, batch_size=90, epochs=15, validation_data=(x_test32, y_test32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--_Ss81s1lI4",
        "outputId": "587adb09-f27b-437c-bd65-c696349790c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 9ms/step - loss: 0.6129 - accuracy: 0.7879\n",
            "Test accuracy with 32 input dimensions: 0.7878788113594055\n"
          ]
        }
      ],
      "source": [
        "test_loss32, test_acc32 = model1.evaluate(x_test32, y_test32)\n",
        "print(f\"Test accuracy with 32 input dimensions: {test_acc32}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Dimention 32 and batch size 150"
      ],
      "metadata": {
        "id": "b29b5KEZhXCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u895vzo_4vof",
        "outputId": "a3dc7426-2045-4bc8-c2cb-e31e80c0d5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "14/14 [==============================] - 3s 42ms/step - loss: 0.6460 - accuracy: 0.8192 - val_loss: 0.7098 - val_accuracy: 0.7455\n",
            "Epoch 2/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.2831 - accuracy: 0.9030 - val_loss: 0.7148 - val_accuracy: 0.7838\n",
            "Epoch 3/15\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 0.1479 - accuracy: 0.9515 - val_loss: 0.6885 - val_accuracy: 0.8121\n",
            "Epoch 4/15\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 0.0932 - accuracy: 0.9717 - val_loss: 0.7725 - val_accuracy: 0.8000\n",
            "Epoch 5/15\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 0.0639 - accuracy: 0.9833 - val_loss: 0.7422 - val_accuracy: 0.8040\n",
            "Epoch 6/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0416 - accuracy: 0.9909 - val_loss: 0.8101 - val_accuracy: 0.8182\n",
            "Epoch 7/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0270 - accuracy: 0.9949 - val_loss: 0.8928 - val_accuracy: 0.8141\n",
            "Epoch 8/15\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 0.0206 - accuracy: 0.9949 - val_loss: 0.9539 - val_accuracy: 0.8121\n",
            "Epoch 9/15\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 0.0136 - accuracy: 0.9980 - val_loss: 0.9929 - val_accuracy: 0.8121\n",
            "Epoch 10/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0119 - accuracy: 0.9975 - val_loss: 1.0373 - val_accuracy: 0.8121\n",
            "Epoch 11/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 1.0504 - val_accuracy: 0.8101\n",
            "Epoch 12/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0053 - accuracy: 0.9995 - val_loss: 1.1699 - val_accuracy: 0.8081\n",
            "Epoch 13/15\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.1601 - val_accuracy: 0.8202\n",
            "Epoch 14/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.1724 - val_accuracy: 0.8202\n",
            "Epoch 15/15\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2367 - val_accuracy: 0.8101\n"
          ]
        }
      ],
      "source": [
        "model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history1_2= model1.fit(x_train32, y_train32, batch_size=150, epochs=15, validation_data=(x_test32, y_test32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Awahi54vof",
        "outputId": "949fed71-6328-4532-e2ad-c3b4f1a0c702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 5ms/step - loss: 1.2367 - accuracy: 0.8101\n",
            "Test accuracy with 32 input dimensions with 150 batch size : 0.8101010322570801\n"
          ]
        }
      ],
      "source": [
        "test_loss32, test_acc32 = model1.evaluate(x_test32, y_test32)\n",
        "print(f\"Test accuracy with 32 input dimensions with 150 batch size : {test_acc32}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTi5MUq7jm2k"
      },
      "source": [
        "##Image Dimention: 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XIQ8G2u96jzl"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AvbSIMy56mpT"
      },
      "outputs": [],
      "source": [
        "for i in buildings:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[1],imgShape[1]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(0)\n",
        "for i in forest:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size=(imgShape[1],imgShape[1]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(1)\n",
        "for i in glacier:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size=(imgShape[1],imgShape[1]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(2)\n",
        "for i in mountain:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size=(imgShape[1],imgShape[1]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(3)\n",
        "for i in sea:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[1],imgShape[1]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(2)\n",
        "for i in street:    \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[1],imgShape[1]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3jBBpP2G8Fnr"
      },
      "outputs": [],
      "source": [
        "data64 = np.array(data)\n",
        "labels64 = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "v-QHlsfm8GQQ"
      },
      "outputs": [],
      "source": [
        "x_train64, x_test64, y_train64, y_test64 = train_test_split(data64, labels64, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdTjNrdgzTjE",
        "outputId": "282f0a28-5f42-46b8-eb95-054275677df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1980 train samples\n",
            "495 test samples\n"
          ]
        }
      ],
      "source": [
        "x_train64 = x_train64.astype('float32')\n",
        "x_test64 = x_test64.astype('float32')\n",
        "x_train64 /= 255\n",
        "x_test64 /= 255\n",
        "\n",
        "print(x_train64.shape[0], 'train samples')\n",
        "print(x_test64.shape[0], 'test samples')\n",
        "\n",
        "# Convert labels to categorical format\n",
        "y_train64 = keras.utils.to_categorical(y_train64, 6)\n",
        "y_test64 = keras.utils.to_categorical(y_test64, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhe26LRiPdfb",
        "outputId": "c05bc474-1aeb-4ed6-d879-885d48b5d4fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.convolutional.conv2d.Conv2D at 0x7f211819d040>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f211819de80>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2119c24760>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f2119c24f70>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7f2119c310a0>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f21181d7970>,\n",
              " <keras.layers.reshaping.flatten.Flatten at 0x7f219012bbe0>,\n",
              " <keras.layers.core.dense.Dense at 0x7f2119c317c0>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21a481f700>,\n",
              " <keras.layers.regularization.dropout.Dropout at 0x7f21a481ff40>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21a481f820>]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(32, (5, 5), input_shape=(64,64,3), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model2.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model2.add(Conv2D(200, (5, 5), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "\n",
        "model2.add(Dense(300, activation='relu'))\n",
        "model2.add(Dense(200, activation='relu'))\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Dense(6, activation=\"softmax\"))\n",
        "model2.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Dimention 64 and batch size 90"
      ],
      "metadata": {
        "id": "06qYa_iRhgM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gshcPyHCuyEG",
        "outputId": "e7b1ab6e-3681-4cd5-b86e-3d6bbf875ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "22/22 [==============================] - 3s 45ms/step - loss: 1.4045 - accuracy: 0.4343 - val_loss: 0.9434 - val_accuracy: 0.6162\n",
            "Epoch 2/15\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8569 - accuracy: 0.6485 - val_loss: 0.7959 - val_accuracy: 0.7273\n",
            "Epoch 3/15\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.6827 - accuracy: 0.7318 - val_loss: 0.6731 - val_accuracy: 0.7556\n",
            "Epoch 4/15\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.6171 - accuracy: 0.7586 - val_loss: 0.6434 - val_accuracy: 0.7677\n",
            "Epoch 5/15\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.5896 - accuracy: 0.7773 - val_loss: 0.5729 - val_accuracy: 0.7899\n",
            "Epoch 6/15\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.5603 - accuracy: 0.7889 - val_loss: 0.5692 - val_accuracy: 0.8141\n",
            "Epoch 7/15\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.4702 - accuracy: 0.8263 - val_loss: 0.5933 - val_accuracy: 0.7899\n",
            "Epoch 8/15\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.4044 - accuracy: 0.8500 - val_loss: 0.7256 - val_accuracy: 0.7495\n",
            "Epoch 9/15\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.3773 - accuracy: 0.8626 - val_loss: 0.5193 - val_accuracy: 0.8384\n",
            "Epoch 10/15\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.2926 - accuracy: 0.8939 - val_loss: 0.5117 - val_accuracy: 0.8384\n",
            "Epoch 11/15\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.2694 - accuracy: 0.9020 - val_loss: 0.5611 - val_accuracy: 0.8000\n",
            "Epoch 12/15\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.2716 - accuracy: 0.9015 - val_loss: 0.5710 - val_accuracy: 0.8040\n",
            "Epoch 13/15\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.2290 - accuracy: 0.9192 - val_loss: 0.6821 - val_accuracy: 0.7778\n",
            "Epoch 14/15\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.1665 - accuracy: 0.9409 - val_loss: 0.6155 - val_accuracy: 0.8162\n",
            "Epoch 15/15\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.1214 - accuracy: 0.9586 - val_loss: 0.7056 - val_accuracy: 0.8101\n"
          ]
        }
      ],
      "source": [
        "# Compile the model2\n",
        "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model2\n",
        "history2 = model2.fit(x_train64, y_train64, batch_size=90, epochs=15, validation_data=(x_test64, y_test64))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxeeo8MuuyEH",
        "outputId": "dd0b89e7-0637-4fc2-fe46-c6a2f129feb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 9ms/step - loss: 0.7056 - accuracy: 0.8101\n",
            "Test accuracy with 64 input dimensions: 0.8101010322570801\n"
          ]
        }
      ],
      "source": [
        "test_loss64, test_acc64 = model2.evaluate(x_test64, y_test64)\n",
        "print(f\"Test accuracy with 64 input dimensions: {test_acc64}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Dimention 64 and batch size 150"
      ],
      "metadata": {
        "id": "I0zsW5Iwhj4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxn8_SiO4eK-",
        "outputId": "236ac29c-8bd2-4c46-c4fd-53923ced08be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "14/14 [==============================] - 3s 64ms/step - loss: 0.4968 - accuracy: 0.8394 - val_loss: 0.6627 - val_accuracy: 0.7677\n",
            "Epoch 2/15\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.1960 - accuracy: 0.9465 - val_loss: 0.6810 - val_accuracy: 0.8263\n",
            "Epoch 3/15\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.0958 - accuracy: 0.9682 - val_loss: 0.7166 - val_accuracy: 0.8263\n",
            "Epoch 4/15\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.0605 - accuracy: 0.9823 - val_loss: 0.7786 - val_accuracy: 0.8283\n",
            "Epoch 5/15\n",
            "14/14 [==============================] - 0s 36ms/step - loss: 0.0367 - accuracy: 0.9914 - val_loss: 0.8533 - val_accuracy: 0.8040\n",
            "Epoch 6/15\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 0.0258 - accuracy: 0.9960 - val_loss: 0.9295 - val_accuracy: 0.8061\n",
            "Epoch 7/15\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.0159 - accuracy: 0.9965 - val_loss: 0.9692 - val_accuracy: 0.8081\n",
            "Epoch 8/15\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.0118 - accuracy: 0.9975 - val_loss: 1.1181 - val_accuracy: 0.7818\n",
            "Epoch 9/15\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.0117 - accuracy: 0.9980 - val_loss: 1.1271 - val_accuracy: 0.8061\n",
            "Epoch 10/15\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 1.1116 - val_accuracy: 0.8242\n",
            "Epoch 11/15\n",
            "14/14 [==============================] - 0s 36ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.1698 - val_accuracy: 0.8121\n",
            "Epoch 12/15\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.2434 - val_accuracy: 0.8121\n",
            "Epoch 13/15\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2386 - val_accuracy: 0.8040\n",
            "Epoch 14/15\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 7.6395e-04 - accuracy: 1.0000 - val_loss: 1.3129 - val_accuracy: 0.8182\n",
            "Epoch 15/15\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 6.3987e-04 - accuracy: 1.0000 - val_loss: 1.3336 - val_accuracy: 0.8101\n"
          ]
        }
      ],
      "source": [
        "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history2_2 = model2.fit(x_train64, y_train64, batch_size=150, epochs=15, validation_data=(x_test64, y_test64))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QzrHHtD4eK_",
        "outputId": "f27d9c7e-8bd6-4794-c1ba-63892103fa7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 5ms/step - loss: 1.3336 - accuracy: 0.8101\n",
            "Test accuracy with 64 input dimensions with 150 batch size: 0.8101010322570801\n"
          ]
        }
      ],
      "source": [
        "test_loss64, test_acc64 = model2.evaluate(x_test64, y_test64)\n",
        "print(f\"Test accuracy with 64 input dimensions with 150 batch size: {test_acc64}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTyA4Jr66zSk"
      },
      "source": [
        "##Image Dimention: 256 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c-yjNDWA6zSl"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "H2bq91X06zSl"
      },
      "outputs": [],
      "source": [
        "for i in buildings:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[2],imgShape[2]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(0)\n",
        "for i in forest:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size=(imgShape[2],imgShape[2]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(1)\n",
        "for i in glacier:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[2],imgShape[2]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(2)\n",
        "for i in mountain:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[2],imgShape[2]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(3)\n",
        "for i in sea:   \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[2],imgShape[2]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(2)\n",
        "for i in street:    \n",
        "    image=tf.keras.preprocessing.image.load_img(i,target_size= (imgShape[2],imgShape[2]))\n",
        "    image=np.array(image)\n",
        "    data.append(image)\n",
        "    labels.append(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LXq_tx5u6zSl"
      },
      "outputs": [],
      "source": [
        "data256 = np.array(data)\n",
        "labels256 = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9JlBJQvl6zSl"
      },
      "outputs": [],
      "source": [
        "x_train256, x_test256, y_train256, y_test256 = train_test_split(data256, labels256, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hean8z4A6zSm",
        "outputId": "79905197-08cf-4c7d-b2b3-ddd7c6f1b957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1980 train samples\n",
            "495 test samples\n"
          ]
        }
      ],
      "source": [
        "x_train256= x_train256.astype('float32')\n",
        "x_test256 = x_test256.astype('float32')\n",
        "x_train256 /= 255\n",
        "x_test256 /= 255\n",
        "\n",
        "print(x_train256.shape[0], 'train samples')\n",
        "print(x_test256.shape[0], 'test samples')\n",
        "\n",
        "# Convert labels to categorical format\n",
        "y_train256 = keras.utils.to_categorical(y_train256, 6)\n",
        "y_test256 = keras.utils.to_categorical(y_test256, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWdAE3sq6zSm",
        "outputId": "31b641ae-a36c-481a-d47f-bbb1d58e02d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1980, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "x_train256.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOnBzAwv6zSm",
        "outputId": "02ec8a85-f2cb-4bd4-ff6b-69a8a5d31513"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.layers.convolutional.conv2d.Conv2D at 0x7f21107b33a0>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f21107288e0>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7f211819d730>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f21107196a0>,\n",
              " <keras.layers.convolutional.conv2d.Conv2D at 0x7f21a48b2160>,\n",
              " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f21104f2190>,\n",
              " <keras.layers.reshaping.flatten.Flatten at 0x7f2190121490>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21104fbcd0>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21a4800bb0>,\n",
              " <keras.layers.regularization.dropout.Dropout at 0x7f21117e89d0>,\n",
              " <keras.layers.core.dense.Dense at 0x7f21117e82e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Conv2D(32, (5, 5), input_shape=(256,256,3), activation='relu', padding='same'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model3.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model3.add(Conv2D(200, (5, 5), activation='relu', padding='same'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n",
        "\n",
        "model3.add(Flatten())\n",
        "\n",
        "\n",
        "model3.add(Dense(300, activation='relu'))\n",
        "model3.add(Dense(200, activation='relu'))\n",
        "model3.add(Dropout(0.2))\n",
        "\n",
        "model3.add(Dense(6, activation=\"softmax\"))\n",
        "model3.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Dimention 256 and batch size 90"
      ],
      "metadata": {
        "id": "KbB7sujths76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJyl-4B36zSm",
        "outputId": "e36ff065-ceba-4f5b-abba-d036acb59d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "22/22 [==============================] - 15s 416ms/step - loss: 1.6419 - accuracy: 0.4480 - val_loss: 0.8895 - val_accuracy: 0.6424\n",
            "Epoch 2/15\n",
            "22/22 [==============================] - 6s 274ms/step - loss: 0.8057 - accuracy: 0.6919 - val_loss: 0.6881 - val_accuracy: 0.7414\n",
            "Epoch 3/15\n",
            "22/22 [==============================] - 6s 276ms/step - loss: 0.6761 - accuracy: 0.7465 - val_loss: 0.6372 - val_accuracy: 0.7758\n",
            "Epoch 4/15\n",
            "22/22 [==============================] - 6s 277ms/step - loss: 0.5652 - accuracy: 0.7894 - val_loss: 0.5682 - val_accuracy: 0.8061\n",
            "Epoch 5/15\n",
            "22/22 [==============================] - 6s 281ms/step - loss: 0.4859 - accuracy: 0.8162 - val_loss: 0.5341 - val_accuracy: 0.8000\n",
            "Epoch 6/15\n",
            "22/22 [==============================] - 7s 339ms/step - loss: 0.3734 - accuracy: 0.8687 - val_loss: 0.5716 - val_accuracy: 0.8081\n",
            "Epoch 7/15\n",
            "22/22 [==============================] - 6s 270ms/step - loss: 0.2636 - accuracy: 0.9061 - val_loss: 0.5027 - val_accuracy: 0.8364\n",
            "Epoch 8/15\n",
            "22/22 [==============================] - 6s 281ms/step - loss: 0.1841 - accuracy: 0.9348 - val_loss: 0.5427 - val_accuracy: 0.8424\n",
            "Epoch 9/15\n",
            "22/22 [==============================] - 6s 277ms/step - loss: 0.1178 - accuracy: 0.9616 - val_loss: 0.7620 - val_accuracy: 0.8061\n",
            "Epoch 10/15\n",
            "22/22 [==============================] - 6s 282ms/step - loss: 0.1100 - accuracy: 0.9641 - val_loss: 0.8043 - val_accuracy: 0.8040\n",
            "Epoch 11/15\n",
            "22/22 [==============================] - 6s 270ms/step - loss: 0.0700 - accuracy: 0.9798 - val_loss: 0.8470 - val_accuracy: 0.8323\n",
            "Epoch 12/15\n",
            "22/22 [==============================] - 6s 280ms/step - loss: 0.0288 - accuracy: 0.9934 - val_loss: 0.9074 - val_accuracy: 0.8202\n",
            "Epoch 13/15\n",
            "22/22 [==============================] - 6s 279ms/step - loss: 0.0381 - accuracy: 0.9879 - val_loss: 1.0211 - val_accuracy: 0.8020\n",
            "Epoch 14/15\n",
            "22/22 [==============================] - 6s 282ms/step - loss: 0.0417 - accuracy: 0.9869 - val_loss: 1.0413 - val_accuracy: 0.8121\n",
            "Epoch 15/15\n",
            "22/22 [==============================] - 6s 274ms/step - loss: 0.0268 - accuracy: 0.9914 - val_loss: 0.9061 - val_accuracy: 0.8404\n"
          ]
        }
      ],
      "source": [
        "# Compile the model3\n",
        "model3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model3\n",
        "history3 = model3.fit(x_train256, y_train256, batch_size=90, epochs=15, validation_data=(x_test256, y_test256))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNAkGLAk6zSm",
        "outputId": "14036556-df9b-4122-dd90-3d5faac5cbc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 57ms/step - loss: 0.9467 - accuracy: 0.7838\n",
            "Test accuracy with 256 input dimensions: 0.7838383913040161\n"
          ]
        }
      ],
      "source": [
        "test_loss256, test_acc256 = model3.evaluate(x_test256, y_test256)\n",
        "print(f\"Test accuracy with 256 input dimensions: {test_acc256}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Image Dimention 256 and batch size 150"
      ],
      "metadata": {
        "id": "F5WxI63ehwGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8UJJXQF7lle",
        "outputId": "c22d840c-d0af-455c-b858-518fcb9ff33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "14/14 [==============================] - 14s 660ms/step - loss: 0.0109 - accuracy: 0.9975 - val_loss: 0.9633 - val_accuracy: 0.8141\n",
            "Epoch 2/15\n",
            "14/14 [==============================] - 6s 446ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.9715 - val_accuracy: 0.8283\n",
            "Epoch 3/15\n",
            "14/14 [==============================] - 6s 431ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 1.0982 - val_accuracy: 0.8222\n",
            "Epoch 4/15\n",
            "14/14 [==============================] - 6s 435ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 1.1108 - val_accuracy: 0.8444\n",
            "Epoch 5/15\n",
            "14/14 [==============================] - 6s 443ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 1.1062 - val_accuracy: 0.8364\n",
            "Epoch 6/15\n",
            "14/14 [==============================] - 6s 436ms/step - loss: 9.5090e-04 - accuracy: 1.0000 - val_loss: 1.1213 - val_accuracy: 0.8424\n",
            "Epoch 7/15\n",
            "14/14 [==============================] - 6s 437ms/step - loss: 6.8362e-04 - accuracy: 1.0000 - val_loss: 1.1738 - val_accuracy: 0.8343\n",
            "Epoch 8/15\n",
            "14/14 [==============================] - 6s 435ms/step - loss: 7.4101e-04 - accuracy: 1.0000 - val_loss: 1.2124 - val_accuracy: 0.8283\n",
            "Epoch 9/15\n",
            "14/14 [==============================] - 6s 443ms/step - loss: 4.0910e-04 - accuracy: 1.0000 - val_loss: 1.2034 - val_accuracy: 0.8323\n",
            "Epoch 10/15\n",
            "14/14 [==============================] - 6s 435ms/step - loss: 2.9714e-04 - accuracy: 1.0000 - val_loss: 1.2192 - val_accuracy: 0.8303\n",
            "Epoch 11/15\n",
            "14/14 [==============================] - 6s 449ms/step - loss: 2.6567e-04 - accuracy: 1.0000 - val_loss: 1.2268 - val_accuracy: 0.8263\n",
            "Epoch 12/15\n",
            "14/14 [==============================] - 6s 448ms/step - loss: 2.3533e-04 - accuracy: 1.0000 - val_loss: 1.2365 - val_accuracy: 0.8263\n",
            "Epoch 13/15\n",
            "14/14 [==============================] - 6s 444ms/step - loss: 2.3024e-04 - accuracy: 1.0000 - val_loss: 1.2374 - val_accuracy: 0.8303\n",
            "Epoch 14/15\n",
            "14/14 [==============================] - 6s 441ms/step - loss: 2.2488e-04 - accuracy: 1.0000 - val_loss: 1.2476 - val_accuracy: 0.8323\n",
            "Epoch 15/15\n",
            "14/14 [==============================] - 6s 445ms/step - loss: 2.5889e-04 - accuracy: 1.0000 - val_loss: 1.2829 - val_accuracy: 0.8303\n"
          ]
        }
      ],
      "source": [
        "# Train the model3\n",
        "history3_2 = model3.fit(x_train256, y_train256, batch_size=150, epochs=15, validation_data=(x_test256, y_test256))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlG1wX003qf4",
        "outputId": "4b8fb6f6-20a0-4f29-9eab-b25b67c0f52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 57ms/step - loss: 1.2829 - accuracy: 0.8303\n",
            "Test accuracy with 256 input dimensions with 150 batch size: 0.8303030133247375\n"
          ]
        }
      ],
      "source": [
        "test_loss256, test_acc256 = model3.evaluate(x_test256, y_test256)\n",
        "print(f\"Test accuracy with 256 input dimensions with 150 batch size: {test_acc256}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Observations and Conclusion**\n",
        "\n",
        "\n",
        "####Test accuracy with 32 input and dimensions and 90 batch size: 0.79\n",
        "\n",
        "####Test accuracy with 32 input dimensions and 150 batch size : 0.81\n",
        "\n",
        "####Test accuracy with 64 input and dimensions and 90 batch size: 0.81\n",
        "\n",
        "####Test accuracy with 64 input dimensions and 150 batch size : 0.81\n",
        "\n",
        "####Test accuracy with 256 input and dimensions and 90 batch size: 0.78\n",
        "\n",
        "####Test accuracy with 256 input dimensions and 150 batch size : 0.83"
      ],
      "metadata": {
        "id": "nSK887N_h_Cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####While training the model I observed that \n",
        "* Larger batch size is giving more accuracy because a larger batch size can help to reduce overfitting, as each batch contains a more representative sample of the overall data distribution. But this is not true in all the cases.\n",
        "\n",
        "* Larger batch size increased the run time. In our case the difference is not much.\n",
        "\n",
        "* Larger input images require more memory to store the intermediate activations and gradients during training, which can be a limiting factor for GPUs with limited memory. This results in longer training time or the need to use smaller batch sizes."
      ],
      "metadata": {
        "id": "TeU9O_kIfEUP"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}